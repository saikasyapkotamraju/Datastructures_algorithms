Given an integer n, return an array ans of length n + 1 such that for each i (0 <= i <= n), ans[i] is the number of 1's in the binary representation of i.

 

Example 1:

Input: n = 2
Output: [0,1,1]
Explanation:
0 --> 0
1 --> 1
2 --> 10
Example 2:

Input: n = 5
Output: [0,1,1,2,1,2]
Explanation:
0 --> 0
1 --> 1
2 --> 10
3 --> 11
4 --> 100
5 --> 101
 

Constraints:

0 <= n <= 105

My solution (O(nlogn)):
class Solution {
    public int helper(int num)
    {
        if(num==0)return 0;
        int count = 0;
        while(num > 0)
        {
            count++;
            num = num & (num-1);
        }
        return count;
    }
    public int[] countBits(int n) {
        int[] ans = new int[n+1];
        for(int i=0; i<=n; i++)
        {
            ans[i] = helper(i);
        }
        return ans;
    }
}

Solution(optimal : O(n))
class Solution {
    public int[] countBits(int n) {
        int[] ans = new int[n+1];
        
        for(int i=0; i<=n; i++)
        {
            if(i==0)
            {
                ans[i] = 0;
                continue;
            }
            if(i==1)
            {
                ans[i] = 1;
            }
            if(i%2 == 0)
            {
                ans[i] = ans[i/2];
            }
            else
            {
                ans[i] = ans[i/2] + 1;
            }
        }
        return ans;

    }
}
